{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "from random import random, seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class vectors_producer:\n",
    "    def __init__(self, addresses):\n",
    "        self.p = 0.1\n",
    "        \n",
    "        self.training_array = []\n",
    "        self.training_labels = []\n",
    "        self.validation_array = []\n",
    "        self.validation_labels = []\n",
    "        \n",
    "        seed(1)\n",
    "        \n",
    "        # iterate through all of the files\n",
    "        for i in range(len(addresses)):\n",
    "            with open (addresses[i], 'r') as f:\n",
    "                data = f.read()\n",
    "             \n",
    "            # iterate through features vectors\n",
    "            data = data.split(\"\\n\")\n",
    "            data.pop()\n",
    "            \n",
    "            # convert feature strings to lists\n",
    "            for j in range(len(data)):\n",
    "                data[j] = data[j].split(\";\")\n",
    "                data[j].pop()                \n",
    "                for k in range(len(data[j])):\n",
    "                    data[j][k] = float(data[j][k])\n",
    "                    \n",
    "                if random() > self.p:\n",
    "                    # append training set\n",
    "                    if len(self.training_array) == 0: \n",
    "                        self.training_array = np.array([data[j][0:56]]) \n",
    "                        if data[j][56] == 0: self.training_labels = np.array([[1, 0, 0, 0, 0, 0, 0, 0]])\n",
    "                        elif data[j][56] == 1: self.training_labels = np.array([[0, 1, 0, 0, 0, 0, 0, 0]])\n",
    "                        elif data[j][56] == 2: self.training_labels = np.array([[0, 0, 1, 0, 0, 0, 0, 0]])\n",
    "                        elif data[j][56] == 3: self.training_labels = np.array([[0, 0, 0, 1, 0, 0, 0, 0]])\n",
    "                        elif data[j][56] == 4: self.training_labels = np.array([[0, 0, 0, 0, 1, 0, 0, 0]])\n",
    "                        elif data[j][56] == 5: self.training_labels = np.array([[0, 0, 0, 0, 0, 1, 0, 0]])\n",
    "                        elif data[j][56] == 6: self.training_labels = np.array([[0, 0, 0, 0, 0, 0, 1, 0]])\n",
    "                        elif data[j][56] == 7: self.training_labels = np.array([[0, 0, 0, 0, 0, 0, 0, 1]])\n",
    "                    else:\n",
    "                        self.training_array = np.append(self.training_array, [data[j][0:56]], axis=0)\n",
    "                        if data[j][56] == 0: self.training_labels = np.append(self.training_labels, [[1, 0, 0, 0, 0, 0, 0, 0]], axis=0)\n",
    "                        elif data[j][56] == 1: self.training_labels = np.append(self.training_labels, [[0, 1, 0, 0, 0, 0, 0, 0]], axis=0)\n",
    "                        elif data[j][56] == 2: self.training_labels = np.append(self.training_labels, [[0, 0, 1, 0, 0, 0, 0, 0]], axis=0)\n",
    "                        elif data[j][56] == 3: self.training_labels = np.append(self.training_labels, [[0, 0, 0, 1, 0, 0, 0, 0]], axis=0)\n",
    "                        elif data[j][56] == 4: self.training_labels = np.append(self.training_labels, [[0, 0, 0, 0, 1, 0, 0, 0]], axis=0)\n",
    "                        elif data[j][56] == 5: self.training_labels = np.append(self.training_labels, [[0, 0, 0, 0, 0, 1, 0, 0]], axis=0)\n",
    "                        elif data[j][56] == 6: self.training_labels = np.append(self.training_labels, [[0, 0, 0, 0, 0, 0, 1, 0]], axis=0)\n",
    "                        elif data[j][56] == 7: self.training_labels = np.append(self.training_labels, [[0, 0, 0, 0, 0, 0, 0, 1]], axis=0)\n",
    "                        \n",
    "                else:\n",
    "                    # append validation set\n",
    "                    if len(self.validation_array) == 0: \n",
    "                        self.validation_array = np.array([data[j][0:56]]) \n",
    "                        if data[j][56] == 0: self.validation_labels = np.array([[1, 0, 0, 0, 0, 0, 0, 0]])\n",
    "                        elif data[j][56] == 1: self.validation_labels = np.array([[0, 1, 0, 0, 0, 0, 0, 0]])\n",
    "                        elif data[j][56] == 2: self.validation_labels = np.array([[0, 0, 1, 0, 0, 0, 0, 0]])\n",
    "                        elif data[j][56] == 3: self.validation_labels = np.array([[0, 0, 0, 1, 0, 0, 0, 0]])\n",
    "                        elif data[j][56] == 4: self.validation_labels = np.array([[0, 0, 0, 0, 1, 0, 0, 0]])\n",
    "                        elif data[j][56] == 5: self.validation_labels = np.array([[0, 0, 0, 0, 0, 1, 0, 0]])\n",
    "                        elif data[j][56] == 6: self.validation_labels = np.array([[0, 0, 0, 0, 0, 0, 1, 0]])\n",
    "                        elif data[j][56] == 7: self.validation_labels = np.array([[0, 0, 0, 0, 0, 0, 0, 1]])\n",
    "                    else:\n",
    "                        self.validation_array = np.append(self.validation_array, [data[j][0:56]], axis=0)\n",
    "                        if data[j][56] == 0: self.validation_labels = np.append(self.validation_labels, [[1, 0, 0, 0, 0, 0, 0, 0]], axis=0)\n",
    "                        elif data[j][56] == 1: self.validation_labels = np.append(self.validation_labels, [[0, 1, 0, 0, 0, 0, 0, 0]], axis=0)\n",
    "                        elif data[j][56] == 2: self.validation_labels = np.append(self.validation_labels, [[0, 0, 1, 0, 0, 0, 0, 0]], axis=0)\n",
    "                        elif data[j][56] == 3: self.validation_labels = np.append(self.validation_labels, [[0, 0, 0, 1, 0, 0, 0, 0]], axis=0)\n",
    "                        elif data[j][56] == 4: self.validation_labels = np.append(self.validation_labels, [[0, 0, 0, 0, 1, 0, 0, 0]], axis=0)\n",
    "                        elif data[j][56] == 5: self.validation_labels = np.append(self.validation_labels, [[0, 0, 0, 0, 0, 1, 0, 0]], axis=0)\n",
    "                        elif data[j][56] == 6: self.validation_labels = np.append(self.validation_labels, [[0, 0, 0, 0, 0, 0, 1, 0]], axis=0)\n",
    "                        elif data[j][56] == 7: self.validation_labels = np.append(self.validation_labels, [[0, 0, 0, 0, 0, 0, 0, 1]], axis=0)\n",
    "                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = {\n",
    "    \"noise\": \"data/raw_data/noise/noise_reference 1.txt\",\n",
    "\n",
    "    \"noise taps 1 features 1\": \"data/features1/noise/taps_noise 1.txt\",\n",
    "    \"noise taps 2 features 1\": \"data/features1/noise/taps_noise 2.txt\",\n",
    "    \"noise taps 3 features 1\": \"data/features1/noise/taps_noise 3.txt\",\n",
    "    \"noise taps 4 features 1\": \"data/features1/noise/taps_noise 4.txt\",\n",
    "\n",
    "    \"noise taps 4 features 2\": \"data/features2/noise/taps_noise 4.txt\",\n",
    "    \"noise taps 4 features 3\": \"data/features3/noise/taps_noise 4.txt\",\n",
    "    \"noise taps 4 features 4\": \"data/features4/noise/taps_noise 4.txt\",\n",
    "    \n",
    "    \n",
    "    # features 4 data\n",
    "    \"A_features4\": \"data/features4/taps 2/A/\",\n",
    "    \"D_alt_features4\": \"data/features4/taps 2/D alt/\",\n",
    "    \"S_features4\": \"data/features4/taps 2/S/\",\n",
    "    \"F_features4\": \"data/features4/taps 2/F/\",\n",
    "\n",
    "    \n",
    "    # features 3 data\n",
    "    \"A_features3\": \"data/features3/taps 2/A/\",\n",
    "    \"D_alt_features3\": \"data/features3/taps 2/D alt/\",\n",
    "    \"S_features3\": \"data/features3/taps 2/S/\",\n",
    "    \"F_features3\": \"data/features3/taps 2/F/\",\n",
    "\n",
    "    \n",
    "    # features 2 data\n",
    "    \"A_features2\": \"data/features2/taps 2/A/\",\n",
    "    \"D_alt_features2\": \"data/features2/taps 2/D alt/\",\n",
    "    \"S_features2\": \"data/features2/taps 2/S/\",\n",
    "    \"F_features2\": \"data/features2/taps 2/F/\",\n",
    "\n",
    "    \n",
    "    # features 1 data\n",
    "    \"index_finger_features1\": \"data/features1/taps/index_finger/\",\n",
    "    \"middle_finger_features1\": \"data/features1/taps/middle_finger/\",\n",
    "    \"ring_finger_features1\": \"data/features1/taps/ring_finger/\",\n",
    "    \"little_finger_features1\": \"data/features1/taps/little_finger/\",\n",
    "    \n",
    "    \"hand_up_features1\": \"data/features1/hand_movement/up/\",\n",
    "    \"hand_down_features1\": \"data/features1/hand_movement/down/\",\n",
    "    \"hand_left_features1\": \"data/features1/hand_movement/left/\",\n",
    "    \"hand_right_features1\": \"data/features1/hand_movement/right/\",\n",
    "    \n",
    "    \"A_features1\": \"data/features1/taps 2/A/\",\n",
    "    \"D_features1\": \"data/features1/taps 2/D/\",\n",
    "    \"D_alt_features1\": \"data/features1/taps 2/D alt/\",\n",
    "    \"E_features1\": \"data/features1/taps 2/E/\",\n",
    "    \"E_new_features1\": \"data/features1/taps 2/E new/\",\n",
    "    \"F_features1\": \"data/features1/taps 2/F/\",\n",
    "    \"R_features1\": \"data/features1/taps 2/R/\",\n",
    "    \"R_new_features1\": \"data/features1/taps 2/R new/\",\n",
    "    \"S_features1\": \"data/features1/taps 2/S/\",\n",
    "    \"W_features1\": \"data/features1/taps 2/W/\",\n",
    "    \"W_new_features1\": \"data/features1/taps 2/W new/\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init and compile NN network\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Dense(56))\n",
    "model.add(layers.Dense(28, activation='sigmoid'))\n",
    "model.add(layers.Dense(8, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.01),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23048, 56)\n",
      "(23048, 8)\n",
      "(2656, 56)\n",
      "(2656, 8)\n"
     ]
    }
   ],
   "source": [
    "input_file_list = []\n",
    "\n",
    "for i in range(1, 61):\n",
    "    if not (i == 5):\n",
    "        input_file = directory[\"A_features1\"] + \"A\" + str(i) + \", features.txt\"\n",
    "        input_file_list.append(input_file)\n",
    "\n",
    "    \n",
    "for i in range(1, 61):\n",
    "    input_file = directory[\"S_features1\"] + \"S\" + str(i) + \", features.txt\"\n",
    "    input_file_list.append(input_file)\n",
    "    \n",
    "    \n",
    "for i in range(1, 121):\n",
    "    input_file = directory[\"D_alt_features1\"] + \"D\" + str(i) + \" alt, features.txt\"\n",
    "    input_file_list.append(input_file)    \n",
    "        \n",
    "    \n",
    "for i in range(1, 61):\n",
    "    input_file = directory[\"F_features1\"] + \"F\" + str(i) + \", features.txt\"\n",
    "    input_file_list.append(input_file)\n",
    "\n",
    "    \n",
    "for i in range(1, 31):\n",
    "    input_file = directory[\"W_features1\"] + \"W\" + str(i) + \", features.txt\"\n",
    "    input_file_list.append(input_file)\n",
    "\n",
    "    \n",
    "for i in range(1, 31):\n",
    "    if not (i == 26):\n",
    "        input_file = directory[\"E_features1\"] + \"E\" + str(i) + \", features.txt\"\n",
    "        input_file_list.append(input_file)\n",
    "\n",
    "    \n",
    "for i in range(1, 31):\n",
    "    input_file = directory[\"R_features1\"] + \"R\" + str(i) + \", features.txt\"\n",
    "    input_file_list.append(input_file)\n",
    "\n",
    "    \n",
    "input_file_list.append(directory[\"noise taps 4 features 1\"])\n",
    "\n",
    "mvectors_producer = vectors_producer(input_file_list)\n",
    "print(mvectors_producer.training_array.shape)\n",
    "print(mvectors_producer.training_labels.shape)\n",
    "print(mvectors_producer.validation_array.shape)\n",
    "print(mvectors_producer.validation_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23048 samples, validate on 2656 samples\n",
      "Epoch 1/10\n",
      "23048/23048 [==============================] - 1s 47us/sample - loss: 0.3597 - accuracy: 0.8740 - val_loss: 0.2761 - val_accuracy: 0.8919\n",
      "Epoch 2/10\n",
      "23048/23048 [==============================] - 1s 33us/sample - loss: 0.2309 - accuracy: 0.9101 - val_loss: 0.2425 - val_accuracy: 0.9059\n",
      "Epoch 3/10\n",
      "23048/23048 [==============================] - 1s 34us/sample - loss: 0.2191 - accuracy: 0.9174 - val_loss: 0.2546 - val_accuracy: 0.9044\n",
      "Epoch 4/10\n",
      "23048/23048 [==============================] - 1s 34us/sample - loss: 0.2199 - accuracy: 0.9171 - val_loss: 0.2327 - val_accuracy: 0.9160\n",
      "Epoch 5/10\n",
      "23048/23048 [==============================] - 1s 37us/sample - loss: 0.2212 - accuracy: 0.9171 - val_loss: 0.2415 - val_accuracy: 0.9115\n",
      "Epoch 6/10\n",
      "23048/23048 [==============================] - 1s 38us/sample - loss: 0.2071 - accuracy: 0.9223 - val_loss: 0.2565 - val_accuracy: 0.9044\n",
      "Epoch 7/10\n",
      "23048/23048 [==============================] - 1s 37us/sample - loss: 0.2069 - accuracy: 0.9222 - val_loss: 0.2114 - val_accuracy: 0.9059\n",
      "Epoch 8/10\n",
      "23048/23048 [==============================] - 1s 33us/sample - loss: 0.2044 - accuracy: 0.9226 - val_loss: 0.2523 - val_accuracy: 0.9104\n",
      "Epoch 9/10\n",
      "23048/23048 [==============================] - 1s 35us/sample - loss: 0.2086 - accuracy: 0.9209 - val_loss: 0.2560 - val_accuracy: 0.9051\n",
      "Epoch 10/10\n",
      "23048/23048 [==============================] - 1s 33us/sample - loss: 0.2198 - accuracy: 0.9167 - val_loss: 0.2511 - val_accuracy: 0.8998\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1eb88ace4e0>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = mvectors_producer.training_array\n",
    "labels = mvectors_producer.training_labels\n",
    "val_data = mvectors_producer.validation_array\n",
    "val_labels = mvectors_producer.validation_labels\n",
    "\n",
    "# train NN network\n",
    "model.fit(data, labels, epochs=10, batch_size=32,\n",
    "          validation_data=(val_data, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/asdf_wer\\assets\n"
     ]
    }
   ],
   "source": [
    "# Export the model to a SavedModel\n",
    "model.save('./models/asdf_wer', save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22096"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_saved_model('./models/asdf_wer')\n",
    "tflite_model = converter.convert()\n",
    "open(\"asdf_wer.tflite\", \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = np.array([[0.008208069620253165,0.13515364904755717,2.358642578125,12.5625,0.018500694861778844,1.5442547558352973,0,-0.005636867088607595,0.06604021186121416,0.798095703125,5.71875,0.004417223808092949,1.7193732765538572,0,-0.006625791139240506,0.019713352024000177,0.039794921875,1.34375,0.0003935985076121795,1.4489318750475562,0,-0.0038568037974683546,0.0548144999129977,0.3221435546875,4.171875,0.0030431502904647435,1.4232756412731158,0,-0.005340189873417721,0.027628350504188236,0.082275390625,2.15625,0.0007731119791666666,1.3911228098427462,0,-0.007416930379746835,0.05043968318334361,0.34173583984375,3.7734375,0.002576779096554487,1.5342036198103477,0,-0.008999208860759493,0.04011632302470095,0.1951904296875,3.34375,0.001629951672676282,1.4850834574730492,0,-0.007021360759493671,0.03988454646943832,0.1619873046875,3.15625,0.001611171624599359,1.3581549064368843,0]])\n",
    "#print(l)\n",
    "model.predict(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1142    6   94    5   10    0    3    3]\n",
      " [   0  170    0    3    0   12    2    5]\n",
      " [  16    0  290    0    0    0    5    0]\n",
      " [   1    0    0  268    0    0    0   11]\n",
      " [  10    0    0    0  202    0    0    0]\n",
      " [   0   18    0    0    0  127    0    0]\n",
      " [   7    1    6    1    6    1   82    1]\n",
      " [   7    3    0   23    0    1    5  109]]\n"
     ]
    }
   ],
   "source": [
    "y_pred=model.predict_classes(val_data)\n",
    "\n",
    "y_label = np.zeros(len(y_pred))\n",
    "for i in range(len(y_pred)):\n",
    "    if val_labels[i][1] == 1: y_label[i] = 1\n",
    "    elif val_labels[i][2] == 1: y_label[i] = 2\n",
    "    elif val_labels[i][3] == 1: y_label[i] = 3\n",
    "    elif val_labels[i][4] == 1: y_label[i] = 4\n",
    "    elif val_labels[i][5] == 1: y_label[i] = 5\n",
    "    elif val_labels[i][6] == 1: y_label[i] = 6\n",
    "    elif val_labels[i][7] == 1: y_label[i] = 7\n",
    "        \n",
    "\n",
    "con_mat = tf.math.confusion_matrix(labels=y_label, predictions=y_pred).numpy()\n",
    "con_mat_norm = np.around(con_mat.astype('float') / con_mat.sum(axis=1)[:, np.newaxis], decimals=2)    \n",
    "\n",
    "print(con_mat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
